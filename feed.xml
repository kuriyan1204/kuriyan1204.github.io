<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://kuriyan1204.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kuriyan1204.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-01T10:26:27+00:00</updated><id>https://kuriyan1204.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">From standard softmax attention to FlashAttention &amp;amp; FlashDecoding (Japanese)</title><link href="https://kuriyan1204.github.io/blog/2025/softmax/" rel="alternate" type="text/html" title="From standard softmax attention to FlashAttention &amp;amp; FlashDecoding (Japanese)"/><published>2025-01-28T22:00:00+00:00</published><updated>2025-01-28T22:00:00+00:00</updated><id>https://kuriyan1204.github.io/blog/2025/softmax</id><content type="html" xml:base="https://kuriyan1204.github.io/blog/2025/softmax/"><![CDATA[<p>このブログポストでは attention 計算がいかに高速化されているのかを step-by-step で説明する。具体的には、標準的な softmax attention からスタートし、FlashAttention と FlashDecoding に至るまでを説明する。</p> <h2 id="ゴール">ゴール</h2> <p><img src="https://crfm.stanford.edu/static/img/posts/2023-10-13-flashdecoding/parallelization.gif" alt="FlashAttention"/></p> <blockquote> <p>FlashAttention のイメージ図。key と value と block-wise に分割されているため、GPU の SM を効率的に利用できる。しかし、key &amp; value の系列長方向（行方向）には並列化がなされておらず、query が少ない場合には効率が悪い。より引用</p> </blockquote> <p><img src="https://crfm.stanford.edu/static/img/posts/2023-10-13-flashdecoding/parallelization_kv.gif" alt="FlashDecoding"/></p> <blockquote> <p>FlashDecoding のイメージ図。同一の query に対して、異なる block にある key と value を並列に計算し、最後に結果をマージすることにより key &amp; value の系列長方向（行方向）への並列化を実現している。より引用</p> </blockquote> <p>これを理解する。そのためには block-wise に分割した attention 計算を、うまくマージする方法を考えれば良い。</p> <h2 id="attention-計算">Attention 計算</h2> <p>Attention は query, key から attention score を計算し、そのスコアで value を重み付けして出力する。</p> <ul> <li>入力: $\bf{Q}, \bf{K}, \bf{V} \in \mathbb{R}^{n \times d}$</li> <li>Attention 計算 <ul> <li>Softmax 前のスコア: $\bf{S} = \bf{Q} \bf{K}^T \in \mathbb{R}^{n \times n}$</li> <li>Attention score: $\bf{P} = \text{softmax}(\bf{S}) \in \mathbb{R}^{n \times n}$ <ul> <li>$\text{softmax}$ は行方向に softmax 計算を行う関数</li> </ul> </li> <li>出力: $\bf{O} = \bf{P} \bf{V} \in \mathbb{R}^{n \times d}$</li> </ul> </li> </ul> <h2 id="gpu-の-memory-hierarchy">GPU の memory hierarchy</h2> <p>WIP</p> <h2 id="naive-な-softmax-計算">Naive な softmax 計算</h2> <p>WIP</p> <h2 id="flashattention">FlashAttention:</h2> <p>WIP</p> <h2 id="flashdecoding-系列長方向への並列化">FlashDecoding: 系列長方向への並列化</h2> <p>WIP</p>]]></content><author><name>Hiroto Kurita</name></author><summary type="html"><![CDATA[Attention calculation is a key component in transformer models. This blog post explains how attention calculation is accelerated, starting from naive softmax attention to FlashAttention and FlashDecoding.]]></summary></entry></feed>